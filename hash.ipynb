{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from src.utils import is_index_exists\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# builds a data-frame\n",
    "def build_dataframe():\n",
    "    if not Path('file_name.csv').exists():\n",
    "        df = pd.DataFrame(columns=['file_name', 'hash_id', 'time_stamp'])\n",
    "        df.to_csv('file_name.csv', index=False)\n",
    "\n",
    "def is_ingested(file_path: str):\n",
    "    csv_path = Path('file_name.csv')\n",
    "    if not is_index_exists():\n",
    "        csv_path.unlink(missing_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        build_dataframe()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    hash_id = compute_file_hash(file_path=file_path)\n",
    "    if hash_id in df['hash_id'].values:\n",
    "        return True \n",
    "    entry = [{\n",
    "        \"file_name\": Path(file_path).name,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"time_stamp\": datetime.datetime.now()\n",
    "    }]\n",
    "    df = pd.concat([df, pd.DataFrame(entry)], ignore_index=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def compute_file_hash(file_path, algorithm='sha256'):\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        while chunk := file.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25e6cb",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "### earlier approach was to keep a track of a csv, and every file that is uplaoded we would maintain a hashvalue with other columns in a csv file (it could be db also)\n",
    "##BUT\n",
    "#### commented out the build_dataframe and is_ingested function as they were not serving the purpose,  \n",
    "#### if the csv is deleted then, repload could happen of the same document,\n",
    "#### if the namespace is deleted in the pinecone, then the same file wont be uploaded that is on the csv wont be uplaode due to hash check\n",
    "##  \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000b54b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7f052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
